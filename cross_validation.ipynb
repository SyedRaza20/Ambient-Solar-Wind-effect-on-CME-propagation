{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454447c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the import statements:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import machine_learning_protocols as ml\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e79da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_donki(file_path):\n",
    "    \"\"\"\n",
    "    Clean the DONKI dataset and return a DataFrame with selected columns.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The expected format for 'time_21_5' is '%m/%d/%y %H:%M'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Error handling in case the file is not there\n",
    "    try:\n",
    "        clean_data = pd.read_excel(file_path)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file at {file_path} does not exist.\")\n",
    "\n",
    "    # get only the data listed above:\n",
    "    clean_data = clean_data[[\"long\", \"lat\", \"speed\", \"half_width\", \"time_21_5\", \"file_name\", \"cme_transit_obs\", \"cme_transit_sim\", \"PE\"]]\n",
    "\n",
    "    # convert the time_21_5 into a datetime object:\n",
    "    clean_data['time_21_5'] = pd.to_datetime(clean_data['time_21_5'], format='%m/%d/%y %H:%M')\n",
    "\n",
    "    # return the resulting data set. This is the cleaned donki data set\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd825ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_omni_data(file_path):\n",
    "    \"\"\"\n",
    "    Cleans OMNI solar wind data from a specified file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the data file.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Cleaned and processed OMNI data, with EPOCH_Time converted to datetime,\n",
    "      bad values replaced, magnetic field magnitude calculated and converted to Gauss.\n",
    "    \n",
    "    Note:\n",
    "    - Assumes bad values are represented as -1.0000000000000001e+31.\n",
    "    - EPOCH_Time format should match '%d-%m-%YT%H:%M:%S.%f'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    sw_ob = pd.read_csv(file_path, delim_whitespace=True)\n",
    "    \n",
    "    # join the EPOCH date and time together and restore the index \n",
    "    sw_ob[\"EPOCH_Time\"] = sw_ob.index + 'T' + sw_ob[\"EPOCH_Time\"]\n",
    "    sw_ob['EPOCH_Time'] = pd.to_datetime(sw_ob['EPOCH_Time'], format='%d-%m-%YT%H:%M:%S.%f')\n",
    "    sw_ob = sw_ob.reset_index(drop=True)\n",
    "\n",
    "    # Convert EPOCH_Time to datetime, assuming EPOCH_Time is correctly formatted\n",
    "    # This might need adjustment based on the actual format in your data\n",
    "    sw_ob['EPOCH_Time'] = pd.to_datetime(sw_ob['EPOCH_Time'], format='%d-%m-%Y %H:%M:%S.%f')\n",
    "\n",
    "    # Cleaning bad values\n",
    "    sw_ob.replace(-1.0000000000000001e+31, np.nan, inplace=True)\n",
    "\n",
    "    # Select only the numeric columns for interpolation\n",
    "    numeric_cols = sw_ob.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    # Interpolate missing values only in numeric columns\n",
    "    sw_ob[numeric_cols] = sw_ob[numeric_cols].interpolate(method='linear')\n",
    "\n",
    "    # Calculate magnitude of the magnetic field\n",
    "    sw_ob[\"B_mag(nT)\"] = np.sqrt(sw_ob[\"BR_(RTN)(nT)\"]**2 + sw_ob[\"BT_(RTN)(nT)\"]**2 + sw_ob[\"BN_(RTN)(nT)\"]**2)\n",
    "    sw_ob.drop(columns=[\"BR_(RTN)(nT)\", \"BT_(RTN)(nT)\", \"BN_(RTN)(nT)\"], inplace=True)\n",
    "\n",
    "    # Convert magnetic field magnitude from nano Tesla to Gauss\n",
    "    sw_ob[\"B_mag(G)\"] = sw_ob[\"B_mag(nT)\"]*1e-5\n",
    "    sw_ob.drop(columns=[\"B_mag(nT)\"], inplace=True)\n",
    "\n",
    "    # Reset index\n",
    "    sw_ob.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return sw_ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c80c4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for processing observed parameters:\n",
    "def process_observed_parameters(sw_ob, cme_data):\n",
    "    \"\"\"\n",
    "    Processes observed solar wind parameters relative to specific times of interest from donki CME data.\n",
    "    \n",
    "    Parameters: \n",
    "    sw_ob (pandas.DataFrame): \n",
    "        Dataframe containing observed solar wind data with columns: EPOCH_Time, BULK_FLOW_SPEED(km/s), \n",
    "        ION_DENSITY(N/cm3), TEMPERATURE(Deg_K), and B_mag(G).\n",
    "        \n",
    "    cme_data (pandas.DataFrame): \n",
    "        Dataframe containing the DONKI CME catalog data, which includes the \"time_21_5\" column that \n",
    "        specifies the time for which solar wind data should be fetched.\n",
    "        \n",
    "    Returns:\n",
    "    new_df (pandas.DataFrame): \n",
    "        A dataframe that contains the solar wind data for the chosen time interval from each time_21_5 value.\n",
    "    \"\"\"\n",
    "    # Boltzmann's constant\n",
    "    k = 1.380649e-16\n",
    "\n",
    "    # These are the rows to grab SW data \n",
    "    all_rows = []\n",
    "\n",
    "    # counter \n",
    "    counter = 0\n",
    "\n",
    "    for _, cme_row in cme_data.iterrows():\n",
    "        # Get the time of interest from the CME data\n",
    "        time = cme_row[\"time_21_5\"]\n",
    "\n",
    "        # Determine the dynamic time_range based on the rounded minimum of observed and simulated transit times\n",
    "        time_range = round(min(cme_row[\"cme_transit_obs\"], cme_row[\"cme_transit_sim\"]))\n",
    "        \n",
    "        # Find the closest time in the observed solar wind data\n",
    "        time_differences = (sw_ob['EPOCH_Time'] - time).abs()\n",
    "        closest_time_idx = time_differences.idxmin()\n",
    "        closest_time = sw_ob.loc[closest_time_idx, 'EPOCH_Time']\n",
    "\n",
    "        # Define the range of times to extract data from\n",
    "        end_time = closest_time + pd.Timedelta(hours=time_range)\n",
    "\n",
    "        # Select the rows within the time range\n",
    "        range_rows = sw_ob[(sw_ob['EPOCH_Time'] >= closest_time) & (sw_ob['EPOCH_Time'] < end_time)].copy()\n",
    "\n",
    "        # Now, for each row in the range, create a new row with the time and parameters\n",
    "        for _, row in range_rows.iterrows():\n",
    "            new_row = {\n",
    "                \"time_21_5\": time,  # this time is from cme_data, kept constant for all rows in this range\n",
    "                \"real_time_value\": row['EPOCH_Time'],  # add the exact observation time\n",
    "                \"B_ob\": row[\"B_mag(G)\"],\n",
    "                \"V_ob\": row[\"BULK_FLOW_SPEED(km/s)\"],\n",
    "                \"n_ob\": row[\"ION_DENSITY(N/cm3)\"],\n",
    "                \"T_ob\": row[\"TEMPERATURE(Deg_K)\"]\n",
    "            }\n",
    "            all_rows.append(new_row)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    # Create a new dataframe from the list of rows\n",
    "    new_df = pd.DataFrame(all_rows)\n",
    "\n",
    "    # calcualte and add the (total pressure = n*k*T + B^2/8*pi) exerted by observed solar wind\n",
    "    new_df[\"total_pressure_ob\"] = new_df[\"n_ob\"]*k*new_df[\"T_ob\"] + (new_df[\"B_ob\"]**2)/(8*np.pi)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22014da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_simulated_parameters(cme_data, data_directory):\n",
    "    \"\"\"\n",
    "    Processes and retrieves simulated solar wind parameters for given CME events based on the minimum of observed\n",
    "    and simulated CME transit times, rounded to the nearest whole hour.\n",
    "\n",
    "    Parameters:\n",
    "    - cme_data (DataFrame): A DataFrame containing data on CME events, including their times, observed transit times,\n",
    "      and simulated transit times.\n",
    "    - data_directory (str): The path to the directory containing the simulation files.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the simulated solar wind parameters for the given CME events.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Boltzman constant (in CGS units)\n",
    "    k = 1.380649e-16\n",
    "    \n",
    "    all_rows = []\n",
    "    \n",
    "    for _, row in cme_data.iterrows():\n",
    "        \n",
    "        # Determine the dynamic time_range based on the rounded minimum of observed and simulated transit times\n",
    "        time_range = round(min(row['cme_transit_obs'], row['cme_transit_sim']))\n",
    "\n",
    "        # Retrieve and clean the file name\n",
    "        file_name = row['file_name'].replace(\" \", \"\")\n",
    "        folder_name = f\"{row['time_21_5'].year}-{row['time_21_5'].month:02d}\"\n",
    "        \n",
    "        # Construct the full path to the simulation file\n",
    "        file_name_full = os.path.join(data_directory, folder_name, str(file_name) + \"_ENLIL_time_line.dat\")\n",
    "\n",
    "        if os.path.exists(file_name_full):\n",
    "            # Read in the simulation data\n",
    "            sim_data = pd.read_csv(file_name_full, delim_whitespace=True, header=None)\n",
    "            sim_data.columns = sim_data.iloc[0]\n",
    "            sim_data = sim_data.drop([0, 1]).reset_index(drop=True)\n",
    "\n",
    "            # Ensure numeric columns are read correctly\n",
    "            for col in ['B_enl', 'V_enl', 'n_enl', 'T_enl']:\n",
    "                sim_data[col] = pd.to_numeric(sim_data[col], errors='coerce')\n",
    "\n",
    "            # Create the timestamp for each data point\n",
    "            sim_data['timestamp'] = pd.to_datetime(sim_data[['year', 'month', 'day', 'hour', 'minute']])\n",
    "            sim_data = sim_data.drop([\"year\", \"month\", \"day\", \"hour\", \"minute\"], axis=1)\n",
    "\n",
    "            # Find the closest time to 'time_21_5'\n",
    "            closest_time_idx = (sim_data['timestamp'] - row['time_21_5']).abs().idxmin()\n",
    "            closest_time = sim_data.loc[closest_time_idx, 'timestamp']\n",
    "\n",
    "            # Initialize the list to collect data points\n",
    "            hourly_data_points = []\n",
    "\n",
    "            for h in range(time_range):  # Use dynamic, rounded time_range\n",
    "                target_time = closest_time + timedelta(hours=h)\n",
    "                # Check if this target time is within the simulation data's range\n",
    "                if target_time <= sim_data['timestamp'].iloc[-1]:\n",
    "                    # Find the index of the row with the timestamp closest to the target_time\n",
    "                    closest_hourly_idx = (sim_data['timestamp'] - target_time).abs().idxmin()\n",
    "                    closest_hourly_time = sim_data.loc[closest_hourly_idx, 'timestamp']\n",
    "\n",
    "                    new_row = {\n",
    "                        \"time_21_5\": row['time_21_5'],\n",
    "                        \"sim_time_value\": closest_hourly_time,\n",
    "                        \"B_sim\": sim_data.loc[closest_hourly_idx, \"B_enl\"] * 1e-5,  # Convert from nT to Gauss\n",
    "                        \"V_sim\": sim_data.loc[closest_hourly_idx, \"V_enl\"],\n",
    "                        \"n_sim\": sim_data.loc[closest_hourly_idx, \"n_enl\"],\n",
    "                        \"T_sim\": sim_data.loc[closest_hourly_idx, \"T_enl\"] * 1000  # Convert from kilo Kelvin to Kelvin\n",
    "                    }\n",
    "                    hourly_data_points.append(new_row)\n",
    "\n",
    "            all_rows.extend(hourly_data_points)\n",
    "\n",
    "    # Create a new DataFrame from the list of rows\n",
    "    new_df = pd.DataFrame(all_rows)\n",
    "\n",
    "    # Calculate and add the total pressure exerted by simulated solar wind\n",
    "    new_df[\"total_pressure_sim\"] = new_df[\"n_sim\"] * k * new_df[\"T_sim\"] + (new_df[\"B_sim\"] ** 2) / (8 * np.pi)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "969d4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def residual_wind(observed_wind, simulated_wind, donki_data):\n",
    "    \"\"\"\n",
    "    Calculate the residual solar wind parameters between observed and simulated data.\n",
    "\n",
    "    Parameters:\n",
    "    observed_wind (pandas DataFrame): \n",
    "        A DataFrame containing observed solar wind parameters; B_ob, V_ob, n_ob, T_ob, and total_pressure_ob.\n",
    "\n",
    "    simulated_wind (pandas DataFrame):\n",
    "        A DataFrame containing simulated solar wind parameters; B_sim, V_sim, n_sim, T_sim, and total_pressure_sim.\n",
    "\n",
    "    donki (pandas DataFrame):\n",
    "        A DataFrame containing the prediction error (PE) for each CME event.\n",
    "\n",
    "    Returns:\n",
    "    cme_data (pandas DataFrame):\n",
    "        A DataFrame containing the residual solar wind parameters; B_diff, V_diff, n_diff, T_diff, and total_pressure_diff.\n",
    "    \"\"\"\n",
    "    # Calculate the residuals, and put them in a new DataFrame\n",
    "    differences = pd.DataFrame({\n",
    "    \"time_21_5\": observed_wind[\"time_21_5\"],\n",
    "    \"B_diff\": np.abs(observed_wind[\"B_ob\"] - simulated_wind[\"B_sim\"]),\n",
    "    \"V_diff\": np.abs(observed_wind[\"V_ob\"] - simulated_wind[\"V_sim\"]),\n",
    "    \"n_diff\": np.abs(observed_wind[\"n_ob\"] - simulated_wind[\"n_sim\"]),\n",
    "    \"T_diff\": np.abs(observed_wind[\"T_ob\"] - simulated_wind[\"T_sim\"]),\n",
    "    \"P_diff\": np.abs(observed_wind[\"total_pressure_ob\"] - simulated_wind[\"total_pressure_sim\"])\n",
    "    })\n",
    "    \n",
    "    # Now I want to take mean by time_21_5 values and add PE column\n",
    "    cme_data = differences.groupby(\"time_21_5\", as_index=False).mean()\n",
    "\n",
    "    # add the columns donki[\"lat\", \"long\", \"speed\", \"half_width\", \"cme_transit_obs\", \"cme_transit_sim\"] to the cme_data\n",
    "    cme_data = cme_data.merge(donki_data[[\"time_21_5\", \"lat\", \"long\", \"speed\", \"half_width\", \"PE\", \"cme_transit_obs\", \"cme_transit_sim\"]], on=\"time_21_5\")\n",
    "\n",
    "    # Return the resulting DataFrame\n",
    "    return cme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27b68f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data, columns_to_exclude):\n",
    "    \"\"\"\n",
    "    This function takes the data set and returns it after normalizing it,\n",
    "    excluding specified columns.\n",
    "    \n",
    "    Params:\n",
    "        data (pd.DataFrame): The original DataFrame.\n",
    "        columns_to_exclude (list): List of column names to exclude from normalization.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with normalized values, excluding the specified columns.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Exclude the columns that you don't want to normalize\n",
    "    data_to_normalize = data.drop(columns=columns_to_exclude)\n",
    "    \n",
    "    # Fit the scaler on the data to normalize and transform it\n",
    "    normalized_data = pd.DataFrame(scaler.fit_transform(data_to_normalize),\n",
    "                                   columns=data_to_normalize.columns,\n",
    "                                   index=data.index)\n",
    "    \n",
    "    # Combine the excluded columns back into the DataFrame\n",
    "    data_normalized = pd.concat([data[columns_to_exclude], normalized_data], axis=1)\n",
    "    \n",
    "    return data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950a006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the knn function:\n",
    "def knn_neighbors(train_features, train_target, test_features):\n",
    "    \"\"\"\n",
    "    Perform K-Nearest Neighbors regression with hyperparameter optimization using grid search.\n",
    "    \n",
    "    The function performs a grid search over a pre-defined range of hyperparameters to find the optimal K-Nearest Neighbors\n",
    "    regressor model. It then predicts target values for the provided test feature data using the best found model.\n",
    "    \"\"\"\n",
    "    # Hyperparameter optimization:\n",
    "    param_grid = {\n",
    "        'n_neighbors': range(1, 30),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    }\n",
    "\n",
    "    # Fit the KNN model to make predictions:\n",
    "    knn_model = KNeighborsRegressor()\n",
    "    grid_search = GridSearchCV(knn_model, param_grid, cv=5)\n",
    "    grid_search.fit(train_features, train_target)\n",
    "    knn_best = grid_search.best_estimator_\n",
    "    \n",
    "    # Make the predictions:\n",
    "    predictions = knn_best.predict(test_features)\n",
    "    \n",
    "    # Optionally, you can return the best estimator and its parameters along with the predictions\n",
    "    return predictions, knn_best\n",
    "\n",
    "# the Support vector machine function:\n",
    "def svm_regression(train_features, train_target, test_features):\n",
    "    \"\"\"\n",
    "    Perform Support Vector Machine regression with hyperparameter optimization using grid search.\n",
    "    \n",
    "    The function performs a grid search over a pre-defined range of hyperparameters to find the optimal Support Vector\n",
    "    Regression model. It then predicts target values for the provided test feature data using the best found model.\n",
    "    \"\"\"\n",
    "    # Hyperparameter optimization:\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100, 1000], \n",
    "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "\n",
    "    # Fit the SVM model to make predictions:\n",
    "    svm_model = SVR()\n",
    "    grid_search = GridSearchCV(svm_model, param_grid, cv=5)\n",
    "    grid_search.fit(train_features, train_target)\n",
    "    svm_best = grid_search.best_estimator_\n",
    "    \n",
    "    # Make the predictions:\n",
    "    predictions = svm_best.predict(test_features)\n",
    "    \n",
    "    return predictions, svm_best\n",
    "\n",
    "# the linear regression function:\n",
    "def linear_regression(train_features, train_target, test_features):\n",
    "    \"\"\"\n",
    "    Perform Linear Regression to predict target values based on input features.\n",
    "    \n",
    "    The function fits a Linear Regression model using the provided training data and predicts target values for the provided test feature data.\n",
    "    \"\"\"\n",
    "    # Fit the Linear Regression model:\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(train_features, train_target)\n",
    "    \n",
    "    # Make the predictions:\n",
    "    predictions = linear_model.predict(test_features)\n",
    "    \n",
    "    return predictions, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758de49",
   "metadata": {},
   "source": [
    "## Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3764b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the DONKI data cleaned:\n",
    "filepath = \"DONKI_new.xlsx\"\n",
    "donki = clean_donki(filepath)\n",
    "\n",
    "# Get the OMNI data cleaned:\n",
    "omni = clean_omni_data(\"OMNI_COHO1HR_MERGED_MAG_PLASMA_137596.txt\")\n",
    "\n",
    "# process the observed and simulated SW parameters:\n",
    "processed_wind_obs = process_observed_parameters(omni, donki)\n",
    "processed_wind_sim = process_simulated_parameters(donki, \"ENLIL_data\")\n",
    "\n",
    "# call residual wind to get the final data :)\n",
    "cme_data = residual_wind(processed_wind_obs, processed_wind_sim, donki)\n",
    "cme_data = normalize_data(cme_data, [\"PE\", \"time_21_5\", \"cme_transit_obs\", \"cme_transit_sim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "531f9c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PE</th>\n",
       "      <th>time_21_5</th>\n",
       "      <th>cme_transit_obs</th>\n",
       "      <th>cme_transit_sim</th>\n",
       "      <th>B_diff</th>\n",
       "      <th>V_diff</th>\n",
       "      <th>n_diff</th>\n",
       "      <th>T_diff</th>\n",
       "      <th>P_diff</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>speed</th>\n",
       "      <th>half_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.2</td>\n",
       "      <td>2012-03-13 18:55:00</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>35.416667</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.715278</td>\n",
       "      <td>0.812868</td>\n",
       "      <td>0.057233</td>\n",
       "      <td>0.006491</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.870748</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.1</td>\n",
       "      <td>2012-07-12 19:35:00</td>\n",
       "      <td>45.850000</td>\n",
       "      <td>38.750000</td>\n",
       "      <td>0.059022</td>\n",
       "      <td>0.130193</td>\n",
       "      <td>0.089135</td>\n",
       "      <td>0.029083</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.538835</td>\n",
       "      <td>0.718310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2</td>\n",
       "      <td>2012-08-04 18:46:00</td>\n",
       "      <td>74.533333</td>\n",
       "      <td>75.700000</td>\n",
       "      <td>0.097985</td>\n",
       "      <td>0.053302</td>\n",
       "      <td>0.234972</td>\n",
       "      <td>0.087069</td>\n",
       "      <td>0.054837</td>\n",
       "      <td>0.306818</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.269417</td>\n",
       "      <td>0.436620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-16.0</td>\n",
       "      <td>2012-08-31 22:43:00</td>\n",
       "      <td>60.666667</td>\n",
       "      <td>44.633333</td>\n",
       "      <td>0.347286</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.243830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129802</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>0.634951</td>\n",
       "      <td>0.859155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-23.4</td>\n",
       "      <td>2012-09-28 01:56:00</td>\n",
       "      <td>68.266667</td>\n",
       "      <td>44.850000</td>\n",
       "      <td>0.048622</td>\n",
       "      <td>0.089849</td>\n",
       "      <td>0.087399</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.782313</td>\n",
       "      <td>0.470874</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>-23.6</td>\n",
       "      <td>2023-02-17 22:08:00</td>\n",
       "      <td>59.733333</td>\n",
       "      <td>36.116667</td>\n",
       "      <td>0.068624</td>\n",
       "      <td>0.215425</td>\n",
       "      <td>0.041094</td>\n",
       "      <td>0.057269</td>\n",
       "      <td>0.029148</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.629612</td>\n",
       "      <td>0.436620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>9.9</td>\n",
       "      <td>2023-02-21 11:33:00</td>\n",
       "      <td>72.583333</td>\n",
       "      <td>82.450000</td>\n",
       "      <td>0.231307</td>\n",
       "      <td>0.304295</td>\n",
       "      <td>0.106243</td>\n",
       "      <td>0.405416</td>\n",
       "      <td>0.216288</td>\n",
       "      <td>0.670455</td>\n",
       "      <td>0.741497</td>\n",
       "      <td>0.132524</td>\n",
       "      <td>0.112676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>13.3</td>\n",
       "      <td>2023-02-25 23:23:00</td>\n",
       "      <td>34.866667</td>\n",
       "      <td>48.216667</td>\n",
       "      <td>0.189282</td>\n",
       "      <td>0.205137</td>\n",
       "      <td>0.478782</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.444807</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.354369</td>\n",
       "      <td>0.619718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>4.7</td>\n",
       "      <td>2023-03-11 00:44:00</td>\n",
       "      <td>75.233333</td>\n",
       "      <td>79.966667</td>\n",
       "      <td>0.212956</td>\n",
       "      <td>0.051024</td>\n",
       "      <td>0.102078</td>\n",
       "      <td>0.067695</td>\n",
       "      <td>0.101677</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.157767</td>\n",
       "      <td>0.450704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>13.4</td>\n",
       "      <td>2023-03-12 23:37:00</td>\n",
       "      <td>52.183333</td>\n",
       "      <td>65.550000</td>\n",
       "      <td>0.114029</td>\n",
       "      <td>0.018082</td>\n",
       "      <td>0.130625</td>\n",
       "      <td>0.189947</td>\n",
       "      <td>0.134617</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.557823</td>\n",
       "      <td>0.292718</td>\n",
       "      <td>0.183099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PE           time_21_5  cme_transit_obs  cme_transit_sim    B_diff  \\\n",
       "0    -6.2 2012-03-13 18:55:00        41.666667        35.416667  0.022057   \n",
       "1    -7.1 2012-07-12 19:35:00        45.850000        38.750000  0.059022   \n",
       "2     1.2 2012-08-04 18:46:00        74.533333        75.700000  0.097985   \n",
       "3   -16.0 2012-08-31 22:43:00        60.666667        44.633333  0.347286   \n",
       "4   -23.4 2012-09-28 01:56:00        68.266667        44.850000  0.048622   \n",
       "..    ...                 ...              ...              ...       ...   \n",
       "117 -23.6 2023-02-17 22:08:00        59.733333        36.116667  0.068624   \n",
       "118   9.9 2023-02-21 11:33:00        72.583333        82.450000  0.231307   \n",
       "119  13.3 2023-02-25 23:23:00        34.866667        48.216667  0.189282   \n",
       "120   4.7 2023-03-11 00:44:00        75.233333        79.966667  0.212956   \n",
       "121  13.4 2023-03-12 23:37:00        52.183333        65.550000  0.114029   \n",
       "\n",
       "       V_diff    n_diff    T_diff    P_diff       lat      long     speed  \\\n",
       "0    0.715278  0.812868  0.057233  0.006491  0.795455  0.870748  1.000000   \n",
       "1    0.130193  0.089135  0.029083  0.008809  0.363636  0.619048  0.538835   \n",
       "2    0.053302  0.234972  0.087069  0.054837  0.306818  0.387755  0.269417   \n",
       "3    0.000966  0.243830  0.000000  0.129802  0.340909  0.149660  0.634951   \n",
       "4    0.089849  0.087399  0.010735  0.004689  0.568182  0.782313  0.470874   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "117  0.215425  0.041094  0.057269  0.029148  0.556818  0.442177  0.629612   \n",
       "118  0.304295  0.106243  0.405416  0.216288  0.670455  0.741497  0.132524   \n",
       "119  0.205137  0.478782  1.000000  0.444807  0.625000  0.904762  0.354369   \n",
       "120  0.051024  0.102078  0.067695  0.101677  0.170455  0.836735  0.157767   \n",
       "121  0.018082  0.130625  0.189947  0.134617  0.079545  0.557823  0.292718   \n",
       "\n",
       "     half_width  \n",
       "0      0.647887  \n",
       "1      0.718310  \n",
       "2      0.436620  \n",
       "3      0.859155  \n",
       "4      1.000000  \n",
       "..          ...  \n",
       "117    0.436620  \n",
       "118    0.112676  \n",
       "119    0.619718  \n",
       "120    0.450704  \n",
       "121    0.183099  \n",
       "\n",
       "[122 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cme_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8e233",
   "metadata": {},
   "source": [
    "## Testing out different kinds of cross-validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd7a6756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tts corrected:  12.97331683554847\n",
      "tts test_set:  13.635135135135133\n"
     ]
    }
   ],
   "source": [
    "# this is gonna be train test split:\n",
    "X = cme_data[[\"B_diff\"]] # the features \n",
    "y = cme_data[\"PE\"] # the target\n",
    "\n",
    "# split the data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Now get the ML results:\n",
    "pred = linear_regression(X_train, y_train, X_test)[0]\n",
    "# convert to series\n",
    "pred_series = pd.Series(pred, index=y_test.index)\n",
    "\n",
    "# print the corrected and the test set values:\n",
    "res = y_test - pred_series\n",
    "print(\"tts corrected: \", np.mean(np.abs(res)))\n",
    "print(\"tts test_set: \", np.mean(np.abs(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff5736c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At event:  0\n",
      "At event:  1\n",
      "At event:  2\n",
      "At event:  3\n",
      "At event:  4\n",
      "At event:  5\n",
      "At event:  6\n",
      "At event:  7\n",
      "At event:  8\n",
      "At event:  9\n",
      "At event:  10\n",
      "At event:  11\n",
      "At event:  12\n",
      "At event:  13\n",
      "At event:  14\n",
      "At event:  15\n",
      "At event:  16\n",
      "At event:  17\n",
      "At event:  18\n",
      "At event:  19\n",
      "At event:  20\n",
      "At event:  21\n",
      "At event:  22\n",
      "At event:  23\n",
      "At event:  24\n",
      "At event:  25\n",
      "At event:  26\n",
      "At event:  27\n",
      "At event:  28\n",
      "At event:  29\n",
      "At event:  30\n",
      "At event:  31\n",
      "At event:  32\n",
      "At event:  33\n",
      "At event:  34\n",
      "At event:  35\n",
      "At event:  36\n",
      "At event:  37\n",
      "At event:  38\n",
      "At event:  39\n",
      "At event:  40\n",
      "At event:  41\n",
      "At event:  42\n",
      "At event:  43\n",
      "At event:  44\n",
      "At event:  45\n",
      "At event:  46\n",
      "At event:  47\n",
      "At event:  48\n",
      "At event:  49\n",
      "At event:  50\n",
      "At event:  51\n",
      "At event:  52\n",
      "At event:  53\n",
      "At event:  54\n",
      "At event:  55\n",
      "At event:  56\n",
      "At event:  57\n",
      "At event:  58\n",
      "At event:  59\n",
      "At event:  60\n",
      "At event:  61\n",
      "At event:  62\n",
      "At event:  63\n",
      "At event:  64\n",
      "At event:  65\n",
      "At event:  66\n",
      "At event:  67\n",
      "At event:  68\n",
      "At event:  69\n",
      "At event:  70\n",
      "At event:  71\n",
      "At event:  72\n",
      "At event:  73\n",
      "At event:  74\n",
      "At event:  75\n",
      "At event:  76\n",
      "At event:  77\n",
      "At event:  78\n",
      "At event:  79\n",
      "At event:  80\n",
      "At event:  81\n",
      "At event:  82\n",
      "At event:  83\n",
      "At event:  84\n",
      "At event:  85\n",
      "At event:  86\n",
      "At event:  87\n",
      "At event:  88\n",
      "At event:  89\n",
      "At event:  90\n",
      "At event:  91\n",
      "At event:  92\n",
      "At event:  93\n",
      "At event:  94\n",
      "At event:  95\n",
      "At event:  96\n",
      "At event:  97\n",
      "At event:  98\n",
      "At event:  99\n",
      "At event:  100\n",
      "At event:  101\n",
      "At event:  102\n",
      "At event:  103\n",
      "At event:  104\n",
      "At event:  105\n",
      "At event:  106\n",
      "At event:  107\n",
      "At event:  108\n",
      "At event:  109\n",
      "At event:  110\n",
      "At event:  111\n",
      "At event:  112\n",
      "At event:  113\n",
      "At event:  114\n",
      "At event:  115\n",
      "At event:  116\n",
      "At event:  117\n",
      "At event:  118\n",
      "At event:  119\n",
      "At event:  120\n",
      "At event:  121\n",
      "LOOCV corrected:  [11.54246548]\n",
      "LOOCV test_set:  12.316393442622951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Define features and target\n",
    "X = cme_data[[\"B_diff\", \"P_diff\", \"speed\"]]\n",
    "y = cme_data[\"PE\"]\n",
    "\n",
    "# Initialize LOOCV\n",
    "loo = LeaveOneOut()\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "# Perform LOOCV\n",
    "count = 0\n",
    "for train_index, test_index in loo.split(X):\n",
    "    print(\"At event: \", count)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train and predict\n",
    "    pred = svm_regression(X_train, y_train, X_test)[0]  # Predict single test point\n",
    "    predictions.append(pred)\n",
    "    true_values.append(y_test.values[0])\n",
    "    count += 1\n",
    "\n",
    "# Convert predictions to Series\n",
    "pred_series = pd.Series(predictions, index=y.index)\n",
    "\n",
    "# Calculate error metrics\n",
    "res = y - pred_series\n",
    "print(\"LOOCV corrected: \", np.mean(np.abs(res)))\n",
    "print(\"LOOCV test_set: \", np.mean(np.abs(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba1399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define features and target\n",
    "X = cme_data[[\"P_diff\"]]\n",
    "y = cme_data[\"PE\"]\n",
    "\n",
    "# Initialize K-Fold Cross-Validation\n",
    "k = 5  # Number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train and predict\n",
    "    pred = svm_regression(X_train, y_train, X_test)[0]\n",
    "    predictions.extend(pred)      # Append predictions\n",
    "    true_values.extend(y_test)    # Append actual values\n",
    "\n",
    "# Convert predictions to Series\n",
    "pred_series = pd.Series(predictions, index=y.index[:len(predictions)])\n",
    "\n",
    "# Calculate error metrics\n",
    "res = pd.Series(true_values) - pred_series\n",
    "print(f\"{k}-Fold corrected: \", np.mean(np.abs(res)))\n",
    "print(f\"{k}-Fold test_set: \", np.mean(np.abs(true_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3631d758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
